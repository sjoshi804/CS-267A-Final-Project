let ACTION = discrete(0.25, 0.25, 0.25, 0.25) in 
let OPTIMAL_NOW = flip exp(reward(ACTION, CURRENT_STATE)) 
let _ = observe(OPTIMAL_NOW) in
let NEXT_ACTION = discrete(0.25, 0.25, 0.25, 0.25) in 
let NEXT_STATE = transition_dynamics(STATE, ACTION) in 
let OPTIMAL_NEXT = flip exp(rewards(NEXT_ACTION, NEXT_STATE))
let _observe(OPTIMAL_NEXT) in
let NEXT_NEXT_ACTION = discrete(0.25, 0.25, 0.25, 0.25) in 
let NEXT_NEXT_STATE = transition_dynamics(NEXT_STATE, NEXT_ACTION) in 
let OPTIMAL_NEXT_NEXT = flip exp(rewards(NEXT_NEXT_ACTION, NEXT_NEXT_STATE))
let _observe(OPTIMAL_NEXT_NEXT) in // need to next next next next and so on ...
ACTION